"use strict";(self.webpackChunkgama=self.webpackChunkgama||[]).push([[6889],{8392:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>r,contentTitle:()=>i,default:()=>h,frontMatter:()=>a,metadata:()=>c,toc:()=>t});var s=o(4848),l=o(8453);const a={sidebar_position:3},i="Running LLMs in Local",c={id:"workshop03/intro",title:"Running LLMs in Local",description:"\u6709\u4e2a\u597d\u673a\u5668\u548c\u597d\u7f51\u7edc\uff01",source:"@site/docs/workshop03/intro.md",sourceDirName:"workshop03",slug:"/workshop03/intro",permalink:"/docs/workshop03/intro",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/workshop03/intro.md",tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_position:3},sidebar:"workshopSidebar",previous:{title:"Second Brain (RAG)",permalink:"/docs/workshop02/intro"}},r={},t=[{value:"Opensource",id:"opensource",level:2},{value:"Huggingface",id:"huggingface",level:3},{value:"Transformers",id:"transformers",level:4},{value:"Model",id:"model",level:4},{value:"datasets",id:"datasets",level:4},{value:"spaces",id:"spaces",level:4},{value:"Load model from Huggingface",id:"load-model-from-huggingface",level:4},{value:"Model Scope \u9b54\u642d\u793e\u533a",id:"model-scope-\u9b54\u642d\u793e\u533a",level:3},{value:"modelscope libary",id:"modelscope-libary",level:4},{value:"setup",id:"setup",level:4},{value:"Load model from Model Scope",id:"load-model-from-model-scope",level:4},{value:"vllm",id:"vllm",level:3},{value:"Ollama",id:"ollama",level:3},{value:"Load model from Ollama",id:"load-model-from-ollama",level:4},{value:"Quantized LLMs",id:"quantized-llms",level:4},{value:"llama.cpp",id:"llamacpp",level:4},{value:"Llama-3-8B-Instruct-GGUF",id:"llama-3-8b-instruct-gguf",level:4},{value:"load from disk",id:"load-from-disk",level:4},{value:"ONNX etc",id:"onnx-etc",level:4},{value:"Load model from Huggingface",id:"load-model-from-huggingface-1",level:4}];function d(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",li:"li",ol:"ol",p:"p",...(0,l.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"running-llms-in-local",children:"Running LLMs in Local"}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:"\u6709\u4e2a\u597d\u673a\u5668\u548c\u597d\u7f51\u7edc\uff01"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"opensource",children:"Opensource"}),"\n",(0,s.jsx)(n.h3,{id:"huggingface",children:"Huggingface"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://huggingface.co/",children:"https://huggingface.co/"})}),"\n",(0,s.jsx)(n.h4,{id:"transformers",children:"Transformers"}),"\n",(0,s.jsx)(n.p,{children:"Transformers provides APIs and tools to easily download and train state-of-the-art pretrained models. Using pretrained models can reduce your compute costs, carbon footprint, and save you the time and resources required to train a model from scratch."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://huggingface.co/docs/transformers/en/index",children:"https://huggingface.co/docs/transformers/en/index"})}),"\n",(0,s.jsx)(n.h4,{id:"model",children:"Model"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://huggingface.co/models",children:"https://huggingface.co/models"})}),"\n",(0,s.jsx)(n.h4,{id:"datasets",children:"datasets"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://huggingface.co/datasets",children:"https://huggingface.co/datasets"})}),"\n",(0,s.jsx)(n.h4,{id:"spaces",children:"spaces"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.a,{href:"https://huggingface.co/spaces",children:"https://huggingface.co/spaces"}),"\n",(0,s.jsx)(n.a,{href:"https://huggingface.co/pricing#spaces",children:"https://huggingface.co/pricing#spaces"})]}),"\n",(0,s.jsx)(n.h4,{id:"load-model-from-huggingface",children:"Load model from Huggingface"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.code,{children:"workshop/03-LLMs-Local/Qwen2-1.5B-Instruct-huggingface.py"})}),"\n",(0,s.jsx)(n.h3,{id:"model-scope-\u9b54\u642d\u793e\u533a",children:"Model Scope \u9b54\u642d\u793e\u533a"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.a,{href:"https://www.modelscope.cn/home",children:"https://www.modelscope.cn/home"}),"\n",(0,s.jsx)(n.a,{href:"https://community.modelscope.cn/",children:"https://community.modelscope.cn/"})]}),"\n",(0,s.jsx)(n.h4,{id:"modelscope-libary",children:"modelscope libary"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://www.modelscope.cn/docs/Quick%20Start",children:"https://www.modelscope.cn/docs/Quick%20Start"})}),"\n",(0,s.jsx)(n.h4,{id:"setup",children:"setup"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["check CUDA version\n",(0,s.jsx)(n.code,{children:"nvidia-smi"})]}),"\n",(0,s.jsxs)(n.li,{children:["pytorch with CUDA\n",(0,s.jsx)(n.a,{href:"https://pytorch.org/get-started/locally/",children:"https://pytorch.org/get-started/locally/"})]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"load-model-from-model-scope",children:"Load model from Model Scope"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.code,{children:"workshop/03-LLMs-Local/Qwen2-1.5B-Instruct.py"})}),"\n",(0,s.jsx)(n.h3,{id:"vllm",children:"vllm"}),"\n",(0,s.jsx)(n.h3,{id:"ollama",children:"Ollama"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://ollama.com/",children:"https://ollama.com/"})}),"\n",(0,s.jsx)(n.h4,{id:"load-model-from-ollama",children:"Load model from Ollama"}),"\n",(0,s.jsx)(n.h4,{id:"quantized-llms",children:"Quantized LLMs"}),"\n",(0,s.jsx)(n.p,{children:"Process of reducing the precision of the model's parameters (weights) from high-precision formats (such as 32-bit floating point) to lower-precision formats (such as 8-bit integers)."}),"\n",(0,s.jsx)(n.p,{children:"FP32 (32-bit floating point): Standard precision used in most training processes.\nFP16 (16-bit floating point): Common in mixed precision training, offering a good balance between speed and accuracy.\nINT8 (8-bit integer): Often used in inference for maximum efficiency."}),"\n",(0,s.jsx)(n.h4,{id:"llamacpp",children:"llama.cpp"}),"\n",(0,s.jsxs)(n.p,{children:["GGUF was developed by @ggerganov who is also the developer of llama.cpp, a popular C/C++ LLM inference framework.\n",(0,s.jsx)(n.a,{href:"https://huggingface.co/docs/hub/en/gguf",children:"https://huggingface.co/docs/hub/en/gguf"})]}),"\n",(0,s.jsx)(n.h4,{id:"llama-3-8b-instruct-gguf",children:"Llama-3-8B-Instruct-GGUF"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF-v2/tree/main",children:"https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF-v2/tree/main"})}),"\n",(0,s.jsx)(n.h4,{id:"load-from-disk",children:"load from disk"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://www.markhneedham.com/blog/2023/10/18/ollama-hugging-face-gguf-models/",children:"https://www.markhneedham.com/blog/2023/10/18/ollama-hugging-face-gguf-models/"})}),"\n",(0,s.jsx)(n.h4,{id:"onnx-etc",children:"ONNX etc"}),"\n",(0,s.jsx)(n.h4,{id:"load-model-from-huggingface-1",children:"Load model from Huggingface"})]})}function h(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>i,x:()=>c});var s=o(6540);const l={},a=s.createContext(l);function i(e){const n=s.useContext(a);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:i(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);