"use strict";(self.webpackChunkgama=self.webpackChunkgama||[]).push([[6889],{8392:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>t,contentTitle:()=>c,default:()=>h,frontMatter:()=>i,metadata:()=>r,toc:()=>l});var s=o(4848),a=o(8453);const i={sidebar_position:3},c="Running LLMs in Local",r={id:"workshop03/intro",title:"Running LLMs in Local",description:"Opensource",source:"@site/docs/workshop03/intro.md",sourceDirName:"workshop03",slug:"/workshop03/intro",permalink:"/docs/workshop03/intro",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/workshop03/intro.md",tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_position:3},sidebar:"workshopSidebar",previous:{title:"Second Brain (RAG)",permalink:"/docs/workshop02/intro"}},t={},l=[{value:"Opensource",id:"opensource",level:2},{value:"Huggingface",id:"huggingface",level:3},{value:"Transformers",id:"transformers",level:4},{value:"Model",id:"model",level:4},{value:"datasets",id:"datasets",level:4},{value:"spaces",id:"spaces",level:4},{value:"Load model from Huggingface",id:"load-model-from-huggingface",level:4},{value:"Model Scope \u9b54\u642d\u793e\u533a",id:"model-scope-\u9b54\u642d\u793e\u533a",level:3},{value:"modelscope libary",id:"modelscope-libary",level:4},{value:"Ollama",id:"ollama",level:3},{value:"Load model from Ollama",id:"load-model-from-ollama",level:4},{value:"Quantized LLMs",id:"quantized-llms",level:4},{value:"llama.cpp",id:"llamacpp",level:4},{value:"Llama-3-8B-Instruct-GGUF",id:"llama-3-8b-instruct-gguf",level:4},{value:"ONNX etc",id:"onnx-etc",level:4},{value:"Load model from Huggingface",id:"load-model-from-huggingface-1",level:4}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",p:"p",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"running-llms-in-local",children:"Running LLMs in Local"}),"\n",(0,s.jsx)(n.h2,{id:"opensource",children:"Opensource"}),"\n",(0,s.jsx)(n.h3,{id:"huggingface",children:"Huggingface"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://huggingface.co/",children:"https://huggingface.co/"})}),"\n",(0,s.jsx)(n.h4,{id:"transformers",children:"Transformers"}),"\n",(0,s.jsx)(n.p,{children:"Transformers provides APIs and tools to easily download and train state-of-the-art pretrained models. Using pretrained models can reduce your compute costs, carbon footprint, and save you the time and resources required to train a model from scratch."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://huggingface.co/docs/transformers/en/index",children:"https://huggingface.co/docs/transformers/en/index"})}),"\n",(0,s.jsx)(n.h4,{id:"model",children:"Model"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://huggingface.co/models",children:"https://huggingface.co/models"})}),"\n",(0,s.jsx)(n.h4,{id:"datasets",children:"datasets"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://huggingface.co/datasets",children:"https://huggingface.co/datasets"})}),"\n",(0,s.jsx)(n.h4,{id:"spaces",children:"spaces"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.a,{href:"https://huggingface.co/spaces",children:"https://huggingface.co/spaces"}),"\n",(0,s.jsx)(n.a,{href:"https://huggingface.co/pricing#spaces",children:"https://huggingface.co/pricing#spaces"})]}),"\n",(0,s.jsx)(n.h4,{id:"load-model-from-huggingface",children:"Load model from Huggingface"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.code,{children:"workshop/03-LLMs-Local/llama-3-8b.py"})}),"\n",(0,s.jsx)(n.h3,{id:"model-scope-\u9b54\u642d\u793e\u533a",children:"Model Scope \u9b54\u642d\u793e\u533a"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.a,{href:"https://www.modelscope.cn/home",children:"https://www.modelscope.cn/home"}),"\n",(0,s.jsx)(n.a,{href:"https://community.modelscope.cn/",children:"https://community.modelscope.cn/"})]}),"\n",(0,s.jsx)(n.h4,{id:"modelscope-libary",children:"modelscope libary"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://www.modelscope.cn/docs/Quick%20Start",children:"https://www.modelscope.cn/docs/Quick%20Start"})}),"\n",(0,s.jsx)(n.h3,{id:"ollama",children:"Ollama"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://ollama.com/",children:"https://ollama.com/"})}),"\n",(0,s.jsx)(n.h4,{id:"load-model-from-ollama",children:"Load model from Ollama"}),"\n",(0,s.jsx)(n.h4,{id:"quantized-llms",children:"Quantized LLMs"}),"\n",(0,s.jsx)(n.p,{children:"Process of reducing the precision of the model's parameters (weights) from high-precision formats (such as 32-bit floating point) to lower-precision formats (such as 8-bit integers)."}),"\n",(0,s.jsx)(n.p,{children:"FP32 (32-bit floating point): Standard precision used in most training processes.\nFP16 (16-bit floating point): Common in mixed precision training, offering a good balance between speed and accuracy.\nINT8 (8-bit integer): Often used in inference for maximum efficiency."}),"\n",(0,s.jsx)(n.h4,{id:"llamacpp",children:"llama.cpp"}),"\n",(0,s.jsxs)(n.p,{children:["GGUF was developed by @ggerganov who is also the developer of llama.cpp, a popular C/C++ LLM inference framework.\n",(0,s.jsx)(n.a,{href:"https://huggingface.co/docs/hub/en/gguf",children:"https://huggingface.co/docs/hub/en/gguf"})]}),"\n",(0,s.jsx)(n.h4,{id:"llama-3-8b-instruct-gguf",children:"Llama-3-8B-Instruct-GGUF"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.a,{href:"https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF-v2/tree/main",children:"https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF-v2/tree/main"})}),"\n",(0,s.jsx)(n.h4,{id:"onnx-etc",children:"ONNX etc"}),"\n",(0,s.jsx)(n.h4,{id:"load-model-from-huggingface-1",children:"Load model from Huggingface"})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>c,x:()=>r});var s=o(6540);const a={},i=s.createContext(a);function c(e){const n=s.useContext(i);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:c(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);