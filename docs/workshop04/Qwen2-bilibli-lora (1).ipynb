{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from transformers import AutoTokenizer, DataCollatorForSeq2Seq, DataCollatorForLanguageModeling, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer, GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_list[0] ['<|im_start|>system\\n你是一个 bilibili 或者 B 站的用户, 你的任务是回复网友的问题<|im_end|>\\n<|im_start|>user\\n\"阿猛，如果你要cos一个角色，你会选择谁呢？\"<|im_end|>\\n<|im_start|>assistant\\n阿猛可以cos大嫂吗 或者缸灿团队来cos个狂飙想看！！[星星眼][星星眼][星星眼]<|im_end|>', '<|im_start|>system\\n你是一个 bilibili 或者 B 站的用户, 你的任务是回复网友的问题<|im_end|>\\n<|im_start|>user\\n\"怎样让中年男同事瞬间变得时尚硬汉？\"<|im_end|>\\n<|im_start|>assistant\\n建议把小缸改造成卡车硬汉：李有田[doge]<|im_end|>']\n",
      "15318\n",
      "[8422, 12066, 861, 10605, 3053, 10479, 10520, 6275, 6014, 1921, 13805, 12496, 242, 8916, 14951, 6610, 9215, 2104, 3079, 14073, 3930, 445, 3305, 12460, 841, 1141, 790, 12890, 4129, 7874, 5230, 4952, 7092, 14286, 8215, 11046, 8488, 5011, 14705, 14461, 10445, 10953, 9040, 9188, 4426, 11036, 7133, 10021, 9468, 192, 13542, 6140, 14438, 10504, 13874, 5634, 8427, 13264, 1499, 4152, 342, 12663, 10859, 5113, 6409, 9105, 4361, 161, 10340, 3328, 4552, 4324, 5222, 4454, 7953, 7040, 7466, 14728, 13803, 8309, 10886, 11638, 13313, 2397, 5912, 12557, 4815, 14875, 6047, 854, 10883, 14985, 5252, 7351, 1772, 13602, 11200, 9805, 8057, 690]\n",
      "100\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['intstruction'],\n",
      "        num_rows: 15218\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['intstruction'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Open the JSONL file\n",
    "train_list =[]\n",
    "# with open('sample.jsonl', 'r') as file:\n",
    "with open('train.jsonl', 'r') as file:\n",
    "\tfor line in file:\n",
    "\t\t# Parse each line as JSON\n",
    "\t\tjson_obj = json.loads(line)\n",
    "\t\t\n",
    "\t\t# Process the JSON object (example: print it)\n",
    "\t\t# print(json_obj)\n",
    "\t\ttrain_list.append(json_obj['text'])\n",
    "\n",
    "# print('train_list', train_list)\n",
    "print('train_list[0]', train_list[0:2])\n",
    "print(len(train_list))\n",
    "\n",
    "test_index_list = random.sample(range(0, len(train_list)-1), 100)\n",
    "print(test_index_list)\n",
    "\n",
    "test_list = [train_list[index] for index in test_index_list]\n",
    "print(len(test_list))\n",
    "\n",
    "for example in test_list:\n",
    "    train_list.remove(example)\n",
    "    \n",
    "data = DatasetDict({'train':Dataset.from_dict({\"intstruction\":train_list}), 'test':Dataset.from_dict({\"intstruction\":test_list})})\n",
    "# data = Dataset.from_dict({\"intstruction\":train_list})\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': ['intstruction'], 'test': ['intstruction']}\n"
     ]
    }
   ],
   "source": [
    "print(data.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151643\n",
      "<|endoftext|>\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "model_path= '/root/autodl-tmp/qwen/Qwen2-7B-Instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n",
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer.decode(tokenizer.pad_token_id))\n",
    "print(tokenizer.pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a6d288637c94a2793bfd5d94eddc974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15218 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "961523d59e6d4e7d89a2bacc5101d154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 15218\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "})\n",
      "{'input_ids': [151644, 8948, 198, 56568, 101909, 20316, 85948, 92313, 28946, 425, 10236, 104, 247, 9370, 20002, 11, 220, 103929, 88802, 20412, 104787, 100744, 103936, 151645, 198, 151644, 872, 198, 1, 99727, 100525, 3837, 102056, 30534, 9407, 46944, 100780, 3837, 102762, 50404, 100165, 101036, 11319, 1, 151645, 198, 151644, 77091, 198, 99727, 100525, 73670, 9407, 26288, 109790, 101037, 92313, 28946, 107347, 103322, 103932, 36407, 9407, 18947, 101231, 103794, 99172, 50930, 31251, 58, 109014, 99246, 1457, 109014, 99246, 1457, 109014, 99246, 60, 151645], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [151644, 8948, 198, 56568, 101909, 20316, 85948, 92313, 28946, 425, 10236, 104, 247, 9370, 20002, 11, 220, 103929, 88802, 20412, 104787, 100744, 103936, 151645, 198, 151644, 872, 198, 1, 99727, 100525, 3837, 102056, 30534, 9407, 46944, 100780, 3837, 102762, 50404, 100165, 101036, 11319, 1, 151645, 198, 151644, 77091, 198, 99727, 100525, 73670, 9407, 26288, 109790, 101037, 92313, 28946, 107347, 103322, 103932, 36407, 9407, 18947, 101231, 103794, 99172, 50930, 31251, 58, 109014, 99246, 1457, 109014, 99246, 1457, 109014, 99246, 60, 151645]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\n你是一个 bilibili 或者 B 站的用户, 你的任务是回复网友的问题<|im_end|>\\n<|im_start|>user\\n\"阿猛，如果你要cos一个角色，你会选择谁呢？\"<|im_end|>\\n<|im_start|>assistant\\n阿猛可以cos大嫂吗 或者缸灿团队来cos个狂飙想看！！[星星眼][星星眼][星星眼]<|im_end|>'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    # extract text\n",
    "    text = examples[\"intstruction\"]\n",
    "\n",
    "    #tokenize and truncate text\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        f\"{text}\",\n",
    "        # return_tensors=\"np\", # return numpy array\n",
    "        add_special_tokens=False\n",
    "    )\n",
    "    # print(text)\n",
    "    # print(tokenized_inputs)\n",
    "    # return {\n",
    "    #     \"input_ids\": tokenized_inputs['input_ids'] + [tokenizer.pad_token_id],\n",
    "    #     \"attention_mask\": tokenized_inputs['attention_mask'] + [1],\n",
    "    #      \"labels\": tokenized_inputs['input_ids'] + [tokenizer.pad_token_id]\n",
    "    # }\n",
    "    return {\n",
    "        \"input_ids\": tokenized_inputs['input_ids'] ,\n",
    "        \"attention_mask\": tokenized_inputs['attention_mask'] ,\n",
    "         \"labels\": tokenized_inputs['input_ids']\n",
    "    }\n",
    "\n",
    "# tokenize training and validation datasets\n",
    "tokenized_data = data.map(tokenize_function, remove_columns=['intstruction'])\n",
    "print(tokenized_data)\n",
    "print(tokenized_data['train'][0])\n",
    "tokenizer.decode(tokenized_data['train'][0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d835819bda548f5984a9c85780d6ae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.bfloat16"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"cuda\",torch_dtype=torch.bfloat16)\n",
    "model.enable_input_require_grads()\n",
    "model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, r=8, target_modules={'q_proj', 'gate_proj', 'v_proj', 'k_proj', 'o_proj', 'up_proj', 'down_proj'}, lora_alpha=32, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)\n"
     ]
    }
   ],
   "source": [
    "# lora config\n",
    "config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, \n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    inference_mode=False, # 训练模式\n",
    "    r=8, # Lora 秩\n",
    "    lora_alpha=32, # Lora alaph，具体作用参见 Lora 原理\n",
    "    lora_dropout=0.1# Dropout 比例\n",
    ")\n",
    "print(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.26434798934534914\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置训练参数\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"/root/autodl-tmp/loraoutput/Qwen2_instruct_lora0715\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    logging_steps=100,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=100, # 为了快速演示，这里设置10，建议你设置成100\n",
    "    learning_rate=1e-4,\n",
    "    save_on_each_node=True,\n",
    "    gradient_checkpointing=True\n",
    ")\n",
    "# print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "你是一个 bilibili 或者 B 站的用户, 你的任务是回复网友的问题<|im_end|>\n",
      "<|im_start|>user\n",
      "\"如果陷入犯罪诱惑，你会如何选择，像【拳影回忆录】里的角色一样吗？\"<|im_end|>\n",
      "<|im_start|>assistant\n",
      "一千个读者里面就有一千个哈姆雷特，个人感觉是主角和同伴在成长的路上被“犯罪”诱惑了，同伴已经迷失在白雾中，自己追上去的时候才发现自己是错的，自己想回头的时候看见同伴的背包也就是背负着的变成骂名，现在已经回不去了“罪名”将自己关进监狱了，自己看着自己的脏手很后悔，同伴也后悔莫及的看着你<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenized_data['train'][200]['input_ids']))\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_data['train'],\n",
    "    eval_dataset=tokenized_data[\"test\"],\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/root/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='395' max='2853' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 395/2853 08:38 < 54:05, 0.76 it/s, Epoch 0.41/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.263800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.157400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.132100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /root/autodl-tmp/qwen/Qwen2-7B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /root/autodl-tmp/qwen/Qwen2-7B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in /root/autodl-tmp/qwen/Qwen2-7B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "mode_path = '/root/autodl-tmp/qwen/Qwen2-7B-Instruct/'\n",
    "lora_path = '/root/autodl-tmp/loraoutput/Qwen2_instruct_lora/checkpoint-20' # 这里改称你的 lora 输出对应 checkpoint 地址\n",
    "\n",
    "# 加载tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(mode_path, trust_remote_code=True)\n",
    "\n",
    "# 加载模型\n",
    "model = AutoModelForCausalLM.from_pretrained(mode_path, device_map=\"auto\",torch_dtype=torch.bfloat16, trust_remote_code=True).eval()\n",
    "\n",
    "# 加载lora权重\n",
    "model = PeftModel.from_pretrained(model, model_id=lora_path)\n",
    "\n",
    "prompt = \"你是谁？\"\n",
    "inputs = tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": \"假设你是皇帝身边的女人--甄嬛。\"},{\"role\": \"user\", \"content\": prompt}],\n",
    "                                       add_generation_prompt=True,\n",
    "                                       tokenize=True,\n",
    "                                       return_tensors=\"pt\",\n",
    "                                       return_dict=True\n",
    "                                       ).to('cuda')\n",
    "\n",
    "\n",
    "gen_kwargs = {\"max_length\": 2500, \"do_sample\": True, \"top_k\": 1}\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, **gen_kwargs)\n",
    "    outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
